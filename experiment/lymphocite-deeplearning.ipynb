{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12979025,"sourceType":"datasetVersion","datasetId":8050343}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ===================================================================\n# BLOCK 1: INSTALLATION AND IMPORTS\n# ===================================================================\n# Install necessary libraries\n!pip install -q segmentation-models-pytorch tqdm albumentations ultralytics\n\nimport os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport cv2\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nimport segmentation_models_pytorch as smp\nfrom ultralytics import YOLO\nfrom tqdm import tqdm\nimport warnings\nimport urllib.request\nwarnings.filterwarnings('ignore')\n\n# Set up device (use GPU if available)\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")\nif torch.cuda.device_count() > 1:\n    print(f\"Using {torch.cuda.device_count()} GPUs\")","metadata":{"_uuid":"732aaf94-e878-4d02-bfd7-44117a4e909f","_cell_guid":"4efe6032-5bd4-42c2-bf49-55ee977fd825","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---\n## Part 1: U-Net Model Training\n---","metadata":{"_uuid":"cd9842cf-885a-455b-967f-d8804c3b46ab","_cell_guid":"5511e70f-ca6a-4004-b4e9-b17f5b57c9a4","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# ===================================================================\n# 1.1: Prepare Dataset and DataLoader for U-Net\n# ===================================================================\nclass CellSegDataset(Dataset):\n    def __init__(self, images_dir, masks_dir, transform=None):\n        self.images_dir = images_dir\n        self.masks_dir = masks_dir\n        self.image_files = sorted(os.listdir(images_dir))\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.image_files)\n\n    def __getitem__(self, idx):\n        img_name = self.image_files[idx]\n        img_path = os.path.join(self.images_dir, img_name)\n        # Corresponding mask name\n        mask_path = os.path.join(self.masks_dir, f\"MASK_{os.path.splitext(img_name)[0]}.png\")\n\n        image = cv2.imread(img_path, cv2.IMREAD_COLOR)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n\n        if self.transform:\n            augmented = self.transform(image=image, mask=mask)\n            image = augmented[\"image\"]\n            mask = augmented[\"mask\"]\n\n        # Convert mask to a tensor (C, H, W)\n        if isinstance(mask, np.ndarray):\n            mask = torch.from_numpy((mask > 0).astype(\"np.int64\"))\n        elif isinstance(mask, torch.Tensor):\n            mask = (mask > 0).long()\n        else:\n            raise TypeError(f\"Unsupported mask type: {type(mask)}\")\n        \n        # Ensure mask has a channel dimension\n        if mask.dim() == 2:\n            mask = mask.unsqueeze(0)\n\n        return image, mask\n\n# Define image augmentations\ntrain_transform = A.Compose([\n    A.Resize(256, 256),\n    A.HorizontalFlip(p=0.5),\n    A.VerticalFlip(p=0.5),\n    A.Rotate(limit=15, p=0.5),\n    A.RandomResizedCrop(size=(256, 256), scale=(0.9, 1.0), p=0.3),\n    A.Affine(scale=(0.95, 1.05), translate_percent=(0.02, 0.02), rotate=(-10, 10), p=0.3),\n    A.Normalize(mean=(0.0, 0.0, 0.0), std=(1.0, 1.0, 1.0)),\n    ToTensorV2()\n])\n\nval_test_transform = A.Compose([\n    A.Resize(256, 256),\n    A.Normalize(mean=(0.0, 0.0, 0.0), std=(1.0, 1.0, 1.0)),\n    ToTensorV2()\n])\n\n# Path to the dataset\nbase_dir = \"/kaggle/input/cell-counting-roboflow-segmentation-masks\"\n\ntrain_dataset = CellSegDataset(\n    images_dir=os.path.join(base_dir, \"train/images\"),\n    masks_dir=os.path.join(base_dir, \"train/masks_binary\"),\n    transform=train_transform\n)\n\nval_dataset = CellSegDataset(\n    images_dir=os.path.join(base_dir, \"valid/images\"),\n    masks_dir=os.path.join(base_dir, \"valid/masks_binary\"),\n    transform=val_test_transform\n)\n\n# Create DataLoaders\ntrain_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=2)\nval_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, num_workers=2)\n\nprint(f\"Train samples: {len(train_dataset)}, Validation samples: {len(val_dataset)}\")","metadata":{"_uuid":"a3c49b08-12da-4c91-8e15-c553f8bee3f3","_cell_guid":"f3d93cb1-115e-4488-8beb-49986719a2d8","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ===================================================================\n# 1.2: Define U-Net Model, Loss, and Training Function\n# ===================================================================\nmodel_unet_train = smp.Unet(\n    encoder_name=\"resnet152\",\n    encoder_weights=\"imagenet\",\n    classes=2,\n    activation=None,\n)\nprint(f\"U-Net Model parameters: {sum(p.numel() for p in model_unet_train.parameters()):,}\")\n\n# --- Define Loss Functions ---\nclass DiceLoss(nn.Module):\n    \"\"\"Dice Loss for segmentation\"\"\"\n    def __init__(self, smooth=1):\n        super(DiceLoss, self).__init__()\n        self.smooth = smooth\n    \n    def forward(self, predictions, targets):\n        predictions = torch.softmax(predictions, dim=1)\n        predictions = predictions[:, 1, :, :] # Only get probabilities for the foreground class\n        targets = targets.float()\n        \n        intersection = (predictions * targets).sum()\n        dice = (2. * intersection + self.smooth) / (predictions.sum() + targets.sum() + self.smooth)\n        \n        return 1 - dice\n\nclass CombinedLoss(nn.Module):\n    \"\"\"Combination of CrossEntropy and Dice Loss\"\"\"\n    def __init__(self, weight_ce=0.5, weight_dice=0.5):\n        super(CombinedLoss, self).__init__()\n        self.weight_ce = weight_ce\n        self.weight_dice = weight_dice\n        self.ce_loss = nn.CrossEntropyLoss()\n        self.dice_loss = DiceLoss()\n    \n    def forward(self, predictions, targets):\n        # CrossEntropy Loss\n        targets_ce = targets.squeeze(1).long() if targets.dim() == 4 else targets.long()\n        ce = self.ce_loss(predictions, targets_ce)\n        \n        # Dice Loss\n        targets_dice = targets.float().squeeze(1) if targets.dim() == 4 else targets.float()\n        dice = self.dice_loss(predictions, targets_dice)\n        return self.weight_ce * ce + self.weight_dice * dice\n\n# --- Training Function ---\ndef train_unet_model(model, train_loader, val_loader, num_epochs=100, learning_rate=1e-3):\n    if torch.cuda.device_count() > 1:\n        model = nn.DataParallel(model)\n    model.to(device)\n    \n    criterion = CombinedLoss()\n    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=5, factor=0.5)\n    \n    train_losses, val_losses = [], []\n    best_val_loss = float('inf')\n    \n    for epoch in range(num_epochs):\n        model.train()\n        train_loss = 0\n        train_pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs} - Train')\n        \n        for images, masks in train_pbar:\n            images, masks = images.to(device), masks.to(device)\n            \n            optimizer.zero_grad()\n            outputs = model(images)\n            loss = criterion(outputs, masks)\n            loss.backward()\n            optimizer.step()\n            \n            train_loss += loss.item()\n            train_pbar.set_postfix({'loss': loss.item()})\n        \n        train_loss /= len(train_loader)\n        train_losses.append(train_loss)\n        \n        model.eval()\n        val_loss = 0\n        val_pbar = tqdm(val_loader, desc=f'Epoch {epoch+1}/{num_epochs} - Validation')\n        \n        with torch.no_grad():\n            for images, masks in val_pbar:\n                images, masks = images.to(device), masks.to(device)\n                outputs = model(images)\n                loss = criterion(outputs, masks)\n                val_loss += loss.item()\n                val_pbar.set_postfix({'loss': loss.item()})\n        \n        val_loss /= len(val_loader)\n        val_losses.append(val_loss)\n        scheduler.step(val_loss)\n        \n        print(f'Epoch {epoch+1}/{num_epochs}: Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}')\n        \n        if val_loss < best_val_loss:\n            best_val_loss = val_loss\n            torch.save(model.state_dict(), 'best_unet_model.pth')\n            print(f'==> Best model saved with validation loss: {val_loss:.4f}')\n            \n    return model, train_losses, val_losses","metadata":{"_uuid":"0b1ce297-6f46-48ac-82d8-e8ed1a1beb8c","_cell_guid":"b43952fe-474e-4a55-8286-c35baf03fafe","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ===================================================================\n# 1.3: Start U-Net Training\n# ===================================================================\nprint(\"Starting U-Net model training...\")\ntrained_model, train_losses, val_losses = train_unet_model(\n    model_unet_train, train_loader, val_loader, num_epochs=100, learning_rate=1e-3\n)\n\n# Plot the training history\nplt.figure(figsize=(10, 5))\nplt.plot(train_losses, label='Train Loss')\nplt.plot(val_losses, label='Validation Loss')\nplt.title('U-Net Training History')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\nplt.grid(True)\nplt.show()","metadata":{"_uuid":"567295ae-841d-4194-a957-ee522822dd4b","_cell_guid":"d5a57958-6055-4af5-bc58-9a1628d705b6","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---\n## Part 2: YOLOv8 Model Training\n---","metadata":{"_uuid":"a0af59b5-fa2b-47a6-8cd7-bdbda5b32322","_cell_guid":"486c7185-abbc-46af-b7e9-13c5ee6e301d","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# ===================================================================\n# 2.1: Start YOLOv8 Training\n# ===================================================================\nmodel_yolo_train = YOLO('yolov8n.pt') # Start from a pre-trained model\n\nprint(\"Starting YOLOv8 model training...\")\nresults = model_yolo_train.train(\n    data='/kaggle/input/cell-counting-roboflow-segmentation-masks/Cell_Counting_dataset_from_roboflow/data.yaml',\n    epochs=100,\n    imgsz=640,\n    name='cell_detection_yolo_model' # Directory name for saving results\n)\nprint(\"YOLOv8 training completed successfully!\")","metadata":{"_uuid":"51ebc8a4-c78d-4ba4-9430-fd73a004fc71","_cell_guid":"35374210-fc80-4f7e-a82b-5159627762cb","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---\n## Part 3: Inference on New Images from URLs\n---","metadata":{"_uuid":"05116803-8e78-41e2-bfd2-6e076659da0b","_cell_guid":"f6662285-a6cd-4deb-b1ce-eeff6e759820","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# ===================================================================\n# 3.1: Load Trained Models\n# ===================================================================\n\n# --- Load U-Net Model ---\nmodel_unet_eval = smp.Unet(\n    encoder_name=\"resnet152\",\n    encoder_weights=None, # No need to reload imagenet weights\n    classes=2,\n    activation=None,\n)\nif torch.cuda.device_count() > 1:\n    model_unet_eval = nn.DataParallel(model_unet_eval)\n# Load the state_dict from the saved file\nmodel_unet_eval.load_state_dict(torch.load(\"best_unet_model.pth\", map_location=device))\nmodel_unet_eval.to(device)\nmodel_unet_eval.eval()\nprint(\"Trained U-Net model loaded successfully for inference.\")\n\n# --- Load YOLO Model ---\nmodel_path_yolo = '/kaggle/working/runs/detect/cell_detection_yolo_model/weights/best.pt'\nmodel_yolo_eval = YOLO(model_path_yolo)\nprint(\"Trained YOLOv8 model loaded successfully for inference.\")","metadata":{"_uuid":"f92a4f30-3339-4970-8f63-d18778dbccfa","_cell_guid":"945ff926-4534-402b-a466-e307bf4130d3","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ===================================================================\n# 3.2: Define Helper and Pipeline Functions for Inference\n# ===================================================================\ndef estimate_average_cell_diameter(image_path):\n    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n    if img is None: return None\n    _, thresh = cv2.threshold(img, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n    num_labels, labels, stats, _ = cv2.connectedComponentsWithStats(thresh, connectivity=8)\n    areas = [stats[i, cv2.CC_STAT_AREA] for i in range(1, num_labels) if 10 < stats[i, cv2.CC_STAT_AREA] < 50000]\n    if not areas: return None\n    return np.sqrt(np.mean(areas) / np.pi) * 2\n\ndef preprocess_image(image):\n    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n    enhanced_contrast = clahe.apply(gray)\n    return cv2.cvtColor(enhanced_contrast, cv2.COLOR_GRAY2RGB)\n\ndef post_process_mask(mask, min_area=25):\n    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))\n    opened_mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel, iterations=1)\n    num_labels, labels, stats, _ = cv2.connectedComponentsWithStats(opened_mask, connectivity=8)\n    cleaned_mask = np.zeros_like(opened_mask)\n    for i in range(1, num_labels):\n        if stats[i, cv2.CC_STAT_AREA] >= min_area:\n            cleaned_mask[labels == i] = 255\n    return cleaned_mask\n\ndef count_objects(binary_mask):\n    contours, _ = cv2.findContours(binary_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n    return len(contours)\n\ndef predict_unet_mask(model, device, image_tensor):\n    model.eval()\n    with torch.no_grad():\n        outputs = model(image_tensor.to(device))\n        probs = torch.softmax(outputs, dim=1)\n        binary = probs[:, 1, :, :] > 0.5\n        return binary.cpu().numpy().astype(np.uint8) * 255\n\ndef unet_counting_pipeline(model, device, image_path, transform, min_area=25):\n    image = cv2.imread(image_path)\n    if image is None: return 0, None, None\n    TARGET_CELL_DIAMETER = 60.0\n    current_diameter = estimate_average_cell_diameter(image_path)\n    if current_diameter is not None and current_diameter > 0:\n        resize_factor = TARGET_CELL_DIAMETER / current_diameter\n        new_size = (int(image.shape[1] * resize_factor), int(image.shape[0] * resize_factor))\n        image = cv2.resize(image, new_size, interpolation=cv2.INTER_AREA)\n    \n    orig_h, orig_w = image.shape[:2]\n    preprocessed_rgb = preprocess_image(image)\n    input_tensor = transform(image=preprocessed_rgb)[\"image\"].unsqueeze(0)\n    \n    pred_mask_batch = predict_unet_mask(model, device, input_tensor)\n    mask_resized = cv2.resize(pred_mask_batch[0], (orig_w, orig_h), interpolation=cv2.INTER_NEAREST)\n    cleaned_mask = post_process_mask(mask_resized, min_area=min_area)\n    num_objects = count_objects(cleaned_mask)\n    return num_objects, cleaned_mask, image\n\ndef count_cells_yolo(model, image_path, conf_threshold=0.45):\n    img = cv2.imread(image_path)\n    if img is None: return 0, None, None\n    results = model(img, verbose=False, conf=conf_threshold)\n    return len(results[0].boxes), img, results\n\ndef visualize_unet_prediction(original_bgr, cleaned_mask, num_objects):\n    contours, _ = cv2.findContours(cleaned_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n    overlay = cv2.drawContours(original_bgr.copy(), contours, -1, (0, 255, 0), 2) # Green contours\n    plt.figure(figsize=(12, 6))\n    plt.subplot(1, 2, 1); plt.imshow(cv2.cvtColor(original_bgr, cv2.COLOR_BGR2RGB)); plt.title('Original Image'); plt.axis('off')\n    plt.subplot(1, 2, 2); plt.imshow(cv2.cvtColor(overlay, cv2.COLOR_BGR2RGB)); plt.title(f'U-Net Count: {num_objects}'); plt.axis('off');\n    plt.show()\n\ndef visualize_yolo_prediction(original_bgr, yolo_results, num_objects):\n    overlay = original_bgr.copy()\n    for box in yolo_results[0].boxes.xyxy.cpu().numpy():\n        x1, y1, x2, y2 = map(int, box[:4])\n        cv2.rectangle(overlay, (x1, y1), (x2, y2), (0, 255, 0), 2) # Green boxes\n    plt.figure(figsize=(12, 6))\n    plt.subplot(1, 2, 1); plt.imshow(cv2.cvtColor(original_bgr, cv2.COLOR_BGR2RGB)); plt.title('Original Image'); plt.axis('off')\n    plt.subplot(1, 2, 2); plt.imshow(cv2.cvtColor(overlay, cv2.COLOR_BGR2RGB)); plt.title(f'YOLOv8 Count: {num_objects}'); plt.axis('off');\n    plt.show()","metadata":{"_uuid":"40561d79-b043-41fa-9a06-4fcff5a6d6e6","_cell_guid":"7346c7ad-59ac-45bf-8b6f-8dc508e7c4d3","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ===================================================================\n# 3.3: Run Inference on a List of Local Image Paths\n# ===================================================================\nimage_paths = [\n    \"/kaggle/input/cell-counting-roboflow-segmentation-masks/test/images/Screenshot-2024-08-20-at-3-40-57-PM_png.rf.51490a3f822ef799797a83f5462ccc9a.jpg\",\n    \"/kaggle/input/cell-counting-roboflow-segmentation-masks/test/images/Screenshot-2024-08-20-at-3-41-22-PM_png.rf.44ac6ca63f784c0490e88c39d69692ed.jpg\",\n    \"/kaggle/input/cell-counting-roboflow-segmentation-masks/test/images/Screenshot-2024-08-20-at-6-10-00-PM_png.rf.988e622803e078ad613cd30a92b68a20.jpg\",\n    \"/kaggle/input/cell-counting-roboflow-segmentation-masks/test/images/Screenshot-2024-08-20-at-6-10-27-PM_png.rf.cf3e3aac50ec0abae4b45cb33971be50.jpg\"\n]\n\nUNET_MIN_CELL_AREA = 20\nYOLO_CONF_THRESHOLD = 0.5\n\nfor i, path in enumerate(image_paths):\n    print(f\"\\n{'='*20} PROCESSING IMAGE {i+1} {'='*20}\")\n    print(f\"Path: {path}\")\n    \n    # Check if the file exists before processing\n    if not os.path.exists(path):\n        print(f\"Error: Image not found at path: {path}\")\n        continue\n        \n    try:\n        # --- Run U-Net Pipeline ---\n        pred_unet, final_mask, unet_img = unet_counting_pipeline(\n            model_unet_eval, device, path, val_test_transform, min_area=UNET_MIN_CELL_AREA\n        )\n        print(f\"U-Net Predicted Count: {pred_unet}\")\n        if unet_img is not None:\n            visualize_unet_prediction(unet_img, final_mask, pred_unet)\n            \n        # --- Run YOLOv8 Pipeline ---\n        pred_yolo, yolo_img, yolo_results = count_cells_yolo(\n            model_yolo_eval, path, conf_threshold=YOLO_CONF_THRESHOLD\n        )\n        print(f\"YOLOv8 Predicted Count: {pred_yolo}\")\n        if yolo_img is not None:\n            visualize_yolo_prediction(yolo_img, yolo_results, pred_yolo)\n\n    except Exception as e:\n        print(f\"Could not process image. Error: {e}\")","metadata":{"_uuid":"bb087251-1852-4612-a2ab-ead02e1d1e4c","_cell_guid":"a5e1e46e-2d45-4aea-bd63-7c220d65116c","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null}]}